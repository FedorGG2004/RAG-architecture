import logging
from datetime import datetime
from config import *
from mcp_client import MCPClient

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RAGSystem:
    def __init__(self, model_name=MODEL_NAME, use_mcp=True):
        self.model_name = model_name
        self.dialog_history = []
        self.use_mcp = use_mcp
        
        if self.use_mcp:
            try:
                self.mcp_client = MCPClient()
                if self.mcp_client.is_server_running():
                    server_info = self.mcp_client.get_server_info()
                    services = server_info.get("services", {})
                    
                    logger.info("üöÄ RAG —Å–∏—Å—Ç–µ–º–∞ —Å –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ú MCP –∫–ª–∏–µ–Ω—Ç–æ–º")
                    logger.info(f"üìä –°–µ—Ä–≤–∏—Å—ã: –ë–î({services.get('vector_db', 'unknown')}), –ú–æ–¥–µ–ª–∏({services.get('llm_models', 'unknown')})")
                    
                    # –ü–æ–∫–∞–∂–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
                    available_models = self.mcp_client.list_models()
                    if available_models:
                        logger.info(f"üìã –ú–æ–¥–µ–ª–∏ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ: {', '.join(available_models)}")
                    else:
                        logger.warning("‚ö†Ô∏è –ù–∞ —Å–µ—Ä–≤–µ—Ä–µ –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
                else:
                    logger.error("‚ùå MCP —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω! –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python mcp_servers/ai_mcp_server.py")
                    self.use_mcp = False
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ MCP –∫–ª–∏–µ–Ω—Ç–∞: {e}")
                self.use_mcp = False
        else:
            from vector_db import VectorStore
            import ollama
            self.vector_db = VectorStore()
            self.ollama_client = ollama.Client()
            logger.info("üîß RAG —Å–∏—Å—Ç–µ–º–∞ —Å –ø—Ä—è–º—ã–º–∏ –≤—ã–∑–æ–≤–∞–º–∏")
    
    def add_initial_knowledge(self):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π —á–µ—Ä–µ–∑ MCP —Å–µ—Ä–≤–µ—Ä"""
        initial_knowledge = [
            "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ —Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö.",
            "Python —è–≤–ª—è–µ—Ç—Å—è –ø–æ–ø—É–ª—è—Ä–Ω—ã–º —è–∑—ã–∫–æ–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.",
            "RAG (Retrieval-Augmented Generation) - —ç—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è —Å–æ—á–µ—Ç–∞–µ—Ç –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞.",
            "–û–ª–ªa–º–∞ - —ç—Ç–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ.",
            "–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Ö—Ä–∞–Ω–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –≤–∏–¥–µ —á–∏—Å–ª–æ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞."
        ]
        
        print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—á–∞–ª—å–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —á–µ—Ä–µ–∑ MCP...")
        
        success_count = 0
        for knowledge in initial_knowledge:
            if self.use_mcp:
                success = self.mcp_client.add_document(
                    knowledge, 
                    {"source": "base_knowledge", "type": "fact"}
                )
                if success:
                    success_count += 1
                    print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ —á–µ—Ä–µ–∑ MCP: {knowledge[:50]}...")
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è: {knowledge[:50]}...")
            else:
                self.vector_db.add_documents(
                    [knowledge], 
                    [{"source": "base_knowledge", "type": "fact"}]
                )
                success_count += 1
        
        print(f"üìä –ò—Ç–æ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {success_count}/{len(initial_knowledge)}")
        
        # –ü–æ–∫–∞–∂–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        if self.use_mcp:
            info = self.mcp_client.get_collection_info()
            print(f"üìà –í –±–∞–∑–µ —Ç–µ–ø–µ—Ä—å: {info.get('document_count', 0)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            # –ü–æ–∫–∞–∂–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ—Ä–≤–µ—Ä–µ
            server_info = self.mcp_client.get_server_info()
            print(f"üîß –°–µ—Ä–≤–µ—Ä: {server_info.get('models_available', 0)} –º–æ–¥–µ–ª–µ–π –¥–æ—Å—Ç—É–ø–Ω–æ")

    def process_query(self, user_query: str) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ MCP —Å–µ—Ä–≤–µ—Ä"""
        logger.info(f"üë§ –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å: {user_query}")
        
        if self.use_mcp:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π RAG pipeline —á–µ—Ä–µ–∑ MCP —Å–µ—Ä–≤–µ—Ä
            result = self.mcp_client.rag_query(
                query=user_query,
                model=self.model_name,
                top_k=3
            )
            
            answer = result.get("answer", "–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞")
            documents_found = result.get("documents_found", 0)
            timing = result.get("timing", {})
            
            logger.info(f"‚úÖ RAG –æ—Ç–≤–µ—Ç: {documents_found} –¥–æ–∫., {timing.get('total', 0)} —Å–µ–∫")
            
        else:
            # –°—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞ —Å –ø—Ä—è–º—ã–º–∏ –≤—ã–∑–æ–≤–∞–º–∏
            relevant_docs = self.vector_db.search_similar(user_query)
            context = "\n".join(relevant_docs) if relevant_docs else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
            
            prompt = f"""–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç.

–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:
{context}

–í–û–ü–†–û–°: {user_query}

–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

–û–¢–í–ï–¢:"""
            
            response = self.ollama_client.generate(
                model=self.model_name,
                prompt=prompt
            )
            answer = response['response'].strip()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é –∏ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        self.dialog_history.extend([f"User: {user_query}", f"Assistant: {answer}"])
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π (—Ç–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–∏–µ –æ—Ç–≤–µ—Ç—ã)
        if self.should_save_to_memory(user_query, answer):
            self.save_to_memory(user_query, answer)
        
        return answer
    
    def should_save_to_memory(self, query: str, response: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Å—Ç–æ–∏—Ç –ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –æ—Ç–≤–µ—Ç –≤ –ø–∞–º—è—Ç—å"""
        if not response or len(response) < 10:
            return False
        
        # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∞—Å—Ç–∏ –ø—Ä–æ–º–ø—Ç–∞
        forbidden_phrases = [
            "—Ä–∞–∑—ä—è—Å–Ω—è—è –æ—Ç–≤–µ—Ç", "–∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π", 
            "–æ—Ç–ø—Ä–∞–≤–ª—è–π –Ω–∞—à –≤–æ–ø—Ä–æ—Å", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ", "–Ω–µ –∑–Ω–∞—é",
            "–∫–æ–Ω—Ç–µ–∫—Å—Ç:", "–≤–æ–ø—Ä–æ—Å:", "–æ—Ç–≤–µ—Ç:"
        ]
        
        response_lower = response.lower()
        if any(phrase in response_lower for phrase in forbidden_phrases):
            return False
            
        # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è
        if any(word in query.lower() for word in ["–ø—Ä–∏–≤–µ—Ç", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π", "hello"]):
            return False
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ—Ç–≤–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π (—Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
        if len(response.split('.')) < 1:
            return False
            
        return True
    
    def save_to_memory(self, query: str, response: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        try:
            facts_to_save = [
                f"–í–æ–ø—Ä–æ—Å: {query}",
                f"–û—Ç–≤–µ—Ç: {response}",
            ]
            
            metadata = {
                "type": "dialog",
                "timestamp": datetime.now().isoformat(),
                "source": "generated"
            }
            
            for fact in facts_to_save:
                if self.use_mcp:
                    self.mcp_client.add_document(fact, metadata)
                else:
                    self.vector_db.add_documents([fact], [metadata])
                    
            logger.info("üíæ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–º—è—Ç—å")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ø–∞–º—è—Ç—å: {e}")
    
    def get_system_info(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ"""
        if self.use_mcp:
            db_info = self.mcp_client.get_collection_info()
            server_info = self.mcp_client.get_server_info()
            models = self.mcp_client.list_models()
            
            doc_count = db_info.get("document_count", 0)
            models_available = server_info.get("models_available", 0)
            
        else:
            db_info = self.vector_db.get_collection_info()
            doc_count = db_info.get("document_count", 0)
            models_available = 1  # —Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
            models = [self.model_name]
            
        return {
            "model": self.model_name,
            "using_mcp": self.use_mcp,
            "dialog_history_length": len(self.dialog_history),
            "documents_in_db": doc_count,
            "models_available": models_available,
            "available_models": models,
            "mcp_available": self.mcp_client.is_server_running() if self.use_mcp else False
        }
    
    def test_model_generation(self, prompt: str = "–ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–æ –æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ") -> str:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ MCP"""
        if self.use_mcp:
            return self.mcp_client.generate_text(prompt, self.model_name)
        else:
            response = self.ollama_client.generate(model=self.model_name, prompt=prompt)
            return response['response']