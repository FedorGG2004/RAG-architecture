import asyncio
import json
import uvicorn
import time
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import chromadb
from sentence_transformers import SentenceTransformer
from pathlib import Path
import logging
import ollama
from ollama import Client as OllamaClient

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–• ====================

class SearchRequest(BaseModel):
    query: str
    top_k: int = 3

class DocumentAddRequest(BaseModel):
    text: str
    metadata: Optional[Dict[str, Any]] = None

class GenerateRequest(BaseModel):
    model: str = "tinyllama:1.1b"
    prompt: str
    options: Optional[Dict[str, Any]] = None

class ChatMessage(BaseModel):
    role: str  # "user", "assistant", "system"
    content: str

class ChatRequest(BaseModel):
    model: str = "tinyllama:1.1b"
    messages: List[ChatMessage]

class RAGRequest(BaseModel):
    query: str
    model: str = "tinyllama:1.1b"
    top_k: int = 3

# ==================== –û–°–ù–û–í–ù–û–ô –°–ï–†–í–ï–† ====================

class AIMCPServer:
    def __init__(self):
        self.app = FastAPI(
            title="AI MCP Server",
            description="–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π MCP-—Å–µ—Ä–≤–µ—Ä –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î –∏ LLM –º–æ–¥–µ–ª–µ–π",
            version="2.0.0"
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
        self._init_vector_db()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞
        self._init_llm_client()
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –º–∞—Ä—à—Ä—É—Ç–æ–≤
        self.setup_routes()
        
        logger.info("‚úÖ AI MCP Server –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–ë–î + –ú–æ–¥–µ–ª–∏)")

    def _init_vector_db(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            base_dir = Path(__file__).parent.parent
            db_path = base_dir / "data" / "chroma_db"
            self.client = chromadb.PersistentClient(path=str(db_path))
            self.collection = self.client.get_or_create_collection("rag_memory")
            self.embedder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            logger.info("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î: {e}")
            raise

    def _init_llm_client(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM –º–æ–¥–µ–ª—è–º–∏"""
        try:
            self.ollama_client = OllamaClient(host='http://localhost:11434')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
            models_response = self.ollama_client.list()
            logger.info(f"üì¶ –û—Ç–≤–µ—Ç –æ—Ç Ollama: {models_response}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç–≤–µ—Ç–∞
            if 'models' in models_response:
                models_list = models_response['models']
                model_count = len(models_list)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º–µ–Ω–∞ –º–æ–¥–µ–ª–µ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ
                self.available_models = []
                for model in models_list:
                    if 'name' in model:
                        self.available_models.append(model['name'])
                    elif 'model' in model:
                        self.available_models.append(model['model'])
                
                logger.info(f"‚úÖ LLM –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –î–æ—Å—Ç—É–ø–Ω–æ –º–æ–¥–µ–ª–µ–π: {model_count}")
                logger.info(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {self.available_models}")
                
            else:
                # –ï—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è
                logger.warning("‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç Ollama")
                self.available_models = []
                
            # –ï—Å–ª–∏ –Ω–µ—Ç –º–æ–¥–µ–ª–µ–π, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            if not hasattr(self, 'available_models'):
                self.available_models = []
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LLM –∫–ª–∏–µ–Ω—Ç–∞: {e}")
            logger.warning("‚ö†Ô∏è  –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω: ollama serve")
            
            # –°–æ–∑–¥–∞–µ–º –∞—Ç—Ä–∏–±—É—Ç –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            self.available_models = []
            
            # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é - —Å–µ—Ä–≤–µ—Ä –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ –º–æ–¥–µ–ª–µ–π
            logger.warning("üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é —Å–µ—Ä–≤–µ—Ä–∞ –±–µ–∑ LLM –º–æ–¥–µ–ª–µ–π")
            
    def setup_routes(self):
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö API —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤"""
        
        # ==================== –ë–ê–ó–û–í–´–ï –≠–ù–î–ü–û–ò–ù–¢–´ ====================
        
        @self.app.get("/")
        async def root():
            return {
                "message": "AI MCP Server is running!",
                "version": "2.0.0",
                "services": ["vector_db", "llm_models"]
            }

        @self.app.get("/health")
        async def health_check():
            db_status = "healthy" if hasattr(self, 'collection') else "unhealthy"
            llm_status = "healthy" if hasattr(self, 'ollama_client') and self.available_models else "unhealthy"
            
            return {
                "status": "healthy",
                "services": {
                    "vector_db": db_status,
                    "llm_models": llm_status
                },
                "models_available": len(self.available_models)
            }

        # ==================== –í–ï–ö–¢–û–†–ù–ê–Ø –ë–î –≠–ù–î–ü–û–ò–ù–¢–´ ====================
        
        @self.app.post("/search", response_model=Dict)
        async def search_documents(request: SearchRequest):
            """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É"""
            start_time = time.time()
            try:
                logger.info(f"üîç –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: '{request.query}'")
                
                # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
                vector_start = time.time()
                query_embedding = self.embedder.encode([request.query]).tolist()
                vector_time = time.time() - vector_start
                
                # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –ø–æ–∏—Å–∫–∞ –≤ –ë–î
                search_start = time.time()
                results = self.collection.query(
                    query_embeddings=query_embedding,
                    n_results=request.top_k
                )
                search_time = time.time() - search_start
                
                documents = results["documents"][0] if results["documents"] else []
                total_time = time.time() - start_time
                
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞ {total_time:.3f} —Å–µ–∫")
                
                return {
                    "documents": documents,
                    "count": len(documents),
                    "query": request.query,
                    "timing": {
                        "total": round(total_time, 3),
                        "vectorization": round(vector_time, 3),
                        "search": round(search_time, 3)
                    }
                }
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
                raise HTTPException(status_code=500, detail=f"Search error: {str(e)}")

        @self.app.post("/add", response_model=Dict)
        async def add_document(request: DocumentAddRequest):
            """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î"""
            try:
                logger.info(f"üíæ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {request.text[:50]}...")
                
                if request.metadata is None:
                    request.metadata = {"source": "mcp_api", "type": "fact"}
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç –≤ –≤–µ–∫—Ç–æ—Ä
                embedding = self.embedder.encode([request.text]).tolist()
                
                # –°–æ–∑–¥–∞–µ–º ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
                doc_id = f"doc_{hash(request.text) % 1000000}"
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                self.collection.add(
                    embeddings=embedding,
                    documents=[request.text],
                    metadatas=[request.metadata],
                    ids=[doc_id]
                )
                
                logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –¥–æ–±–∞–≤–ª–µ–Ω —Å ID: {doc_id}")
                
                return {
                    "success": True,
                    "message": "–î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω",
                    "doc_id": doc_id,
                    "text_length": len(request.text)
                }
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è: {e}")
                raise HTTPException(status_code=500, detail=f"Add error: {str(e)}")

        @self.app.get("/info")
        async def get_collection_info():
            """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
            try:
                count = self.collection.count()
                return {
                    "document_count": count,
                    "collection_name": "rag_memory",
                    "status": "active"
                }
            except Exception as e:
                raise HTTPException(status_code=500, detail=f"Info error: {str(e)}")

        # ==================== LLM –ú–û–î–ï–õ–ò –≠–ù–î–ü–û–ò–ù–¢–´ ====================
        
        @self.app.post("/generate", response_model=Dict)
        async def generate_text(request: GenerateRequest):
            """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ LLM –º–æ–¥–µ–ª—å"""
            start_time = time.time()
            try:
                logger.info(f"ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª—å—é: {request.model}")
                
                if request.model not in self.available_models:
                    raise HTTPException(status_code=400, detail=f"–ú–æ–¥–µ–ª—å {request.model} –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")
                
                response = self.ollama_client.generate(
                    model=request.model,
                    prompt=request.prompt,
                    options=request.options or {}
                )
                
                generation_time = time.time() - start_time
                logger.info(f"‚úÖ –¢–µ–∫—Å—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {generation_time:.3f} —Å–µ–∫")
                
                return {
                    "response": response['response'],
                    "model": request.model,
                    "prompt_length": len(request.prompt),
                    "generation_time": round(generation_time, 3),
                    "total_duration": response.get('total_duration', 0)
                }
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                raise HTTPException(status_code=500, detail=f"Generation error: {str(e)}")

        @self.app.post("/chat", response_model=Dict)
        async def chat_completion(request: ChatRequest):
            """–ß–∞—Ç-–∫–æ–º–ø–ª–∏—à–Ω —á–µ—Ä–µ–∑ LLM –º–æ–¥–µ–ª—å"""
            start_time = time.time()
            try:
                logger.info(f"üí¨ –ß–∞—Ç-–∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏: {request.model}")
                
                if request.model not in self.available_models:
                    raise HTTPException(status_code=400, detail=f"–ú–æ–¥–µ–ª—å {request.model} –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")
                
                response = self.ollama_client.chat(
                    model=request.model,
                    messages=[{"role": msg.role, "content": msg.content} for msg in request.messages]
                )
                
                chat_time = time.time() - start_time
                logger.info(f"‚úÖ –ß–∞—Ç-–æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {chat_time:.3f} —Å–µ–∫")
                
                return {
                    "message": response['message'],
                    "model": request.model,
                    "chat_time": round(chat_time, 3)
                }
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á–∞—Ç–∞: {e}")
                raise HTTPException(status_code=500, detail=f"Chat error: {str(e)}")

        @self.app.get("/models")
        async def list_models():
            """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
            try:
                models_response = self.ollama_client.list()
                
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –º–æ–¥–µ–ª–∏
                if 'models' in models_response:
                    models_list = models_response['models']
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ
                    formatted_models = []
                    for model in models_list:
                        model_info = {
                            'name': model.get('name', model.get('model', 'unknown')),
                            'size': model.get('size', 0),
                            'modified': model.get('modified_at', '')
                        }
                        formatted_models.append(model_info)
                        
                    return {
                        "models": formatted_models,
                        "count": len(formatted_models)
                    }
                else:
                    return {
                        "models": [],
                        "count": 0,
                        "warning": "–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç Ollama"
                    }
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
                return {
                    "models": [],
                    "count": 0,
                    "error": str(e)
                }

        # ==================== RAG –≠–ù–î–ü–û–ò–ù–¢–´ ====================
        
        @self.app.post("/rag", response_model=Dict)
        async def rag_query(request: RAGRequest):
            """–ü–æ–ª–Ω—ã–π RAG pipeline: –ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è"""
            start_time = time.time()
            try:
                logger.info(f"üéØ RAG –∑–∞–ø—Ä–æ—Å: '{request.query}'")
                
                # 1. –ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
                search_start = time.time()
                query_embedding = self.embedder.encode([request.query]).tolist()
                results = self.collection.query(
                    query_embeddings=query_embedding,
                    n_results=request.top_k
                )
                search_time = time.time() - search_start
                
                documents = results["documents"][0] if results["documents"] else []
                
                # 2. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
                context = "\n".join(documents) if documents else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
                
                prompt = f"""–ò—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. 
–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –Ω—É–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å–∫–∞–∂–∏ —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—à—å.

–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}

–í–æ–ø—Ä–æ—Å: {request.query}

–û—Ç–≤–µ—Ç:"""
                
                # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
                gen_start = time.time()
                response = self.ollama_client.generate(
                    model=request.model,
                    prompt=prompt
                )
                gen_time = time.time() - gen_start
                
                total_time = time.time() - start_time
                
                logger.info(f"‚úÖ RAG –æ—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {total_time:.3f} —Å–µ–∫")
                
                return {
                    "answer": response['response'],
                    "documents_found": len(documents),
                    "model": request.model,
                    "query": request.query,
                    "timing": {
                        "total": round(total_time, 3),
                        "search": round(search_time, 3),
                        "generation": round(gen_time, 3)
                    }
                }
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ RAG: {e}")
                raise HTTPException(status_code=500, detail=f"RAG error: {str(e)}")

        @self.app.post("/batch_add")
        async def batch_add_documents(documents: List[DocumentAddRequest]):
            """–ü–∞–∫–µ—Ç–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
            try:
                texts = [doc.text for doc in documents]
                metadatas = [doc.metadata or {"source": "batch_mcp", "type": "fact"} for doc in documents]
                
                # –ü–∞–∫–µ—Ç–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
                embeddings = self.embedder.encode(texts).tolist()
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ID
                doc_ids = [f"batch_{hash(text) % 1000000}" for text in texts]
                
                # –ü–∞–∫–µ—Ç–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ
                self.collection.add(
                    embeddings=embeddings,
                    documents=texts,
                    metadatas=metadatas,
                    ids=doc_ids
                )
                
                return {
                    "success": True,
                    "message": f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
                    "count": len(documents)
                }
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=f"Batch add error: {str(e)}")

def main():
    """–ó–∞–ø—É—Å–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ MCP —Å–µ—Ä–≤–µ—Ä–∞"""
    try:
        server = AIMCPServer()
        
        print("üöÄ –ó–∞–ø—É—Å–∫ AI MCP Server –Ω–∞ http://localhost:8000")
        print("üìö –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã:")
        print("   ‚îú‚îÄ‚îÄ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î (ChromaDB)")
        print("   ‚îú‚îÄ‚îÄ LLM –ú–æ–¥–µ–ª–∏ (Ollama)") 
        print("   ‚îî‚îÄ‚îÄ RAG Pipeline")
        print("")
        print("üåê API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://localhost:8000/docs")
        print("‚ù§Ô∏è  –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è: http://localhost:8000/health")
        print("")
        print("‚ö° –î–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C")
        print("-" * 50)
        
        uvicorn.run(
            server.app,
            host="0.0.0.0",
            port=8000,
            log_level="info"
        )
    except Exception as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–µ—Ä: {e}")
        print("üîß –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ:")
        print("   - Ollama –∑–∞–ø—É—â–µ–Ω: ollama serve")
        print("   - ChromaDB –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω–∞")
        print("   - –ü–æ—Ä—Ç 8000 —Å–≤–æ–±–æ–¥–µ–Ω")

if __name__ == "__main__":
    main()